[
  {
    "prompt": "What is the capital city of Japan?",
    "completion": "The capital city of Japan is **Tokyo**. It is also the most populous city in Japan and serves as the political, economic, and cultural center of the country. While Kyoto was the imperial capital during the Edo period and remains a significant historical and cultural hub, Tokyo has been the de facto capital since the Meiji Restoration in 1868.",
    "tokens_prompt": 18,
    "tokens_completion": 262,
    "duration_s": 17.756,
    "response_time_s": 17.756,
    "energy_wh_raw": 0.132194,
    "energy_wh_net": 0.132194,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q1",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A one\u2011word answer naming Japan\u2019s capital (Tokyo).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Very short prompt and answer provide a baseline for minimal energy consumption.",
    "tags": [
      "energy-baseline",
      "short-context",
      "factual"
    ],
    "wh_per_1k_tokens": 0.472121,
    "energy_kwh_per_token": 4.72e-07,
    "g_co2": 0.046268
  },
  {
    "prompt": "Calculate 7 + 5.",
    "completion": "The sum of 7 and 5 is calculated by adding the two numbers together. \n\n**Step-by-Step Explanation:**\n1. Start with the number 7.\n2. Add 5 to it: $ 7 + 5 $.\n3. Break down 5 into 2 and 3 for easier addition:\n   - $ 7 + 2 = 9 $\n   - $ 9 + 3 = 12 $\n4. Alternatively, count sequentially from 7: $ 7, 8, 9, 10, 11, 12 $ (5 steps).\n5. Verification via subtraction: $ 12 - 5 = 7 $ and $ 12 - 7 = 5 $, confirming the result.\n\n**Final Answer:**  \n$ \\boxed{12} $",
    "tokens_prompt": 17,
    "tokens_completion": 733,
    "duration_s": 34.208,
    "response_time_s": 34.208,
    "energy_wh_raw": 0.32245,
    "energy_wh_net": 0.32245,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q2",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single number representing the sum (12).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Simple arithmetic with minimal computation and output length.",
    "tags": [
      "numeric",
      "low-compute",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.429933,
    "energy_kwh_per_token": 4.3e-07,
    "g_co2": 0.112857
  },
  {
    "prompt": "Name the largest planet in our solar system.",
    "completion": "The largest planet in our solar system is **Jupiter**. It is a gas giant with a diameter of about 139,822 kilometers (86,881 miles), making it approximately 11 times wider than Earth. Jupiter is known for its prominent Great Red Spot, a massive storm that has been raging for centuries, and its numerous moons, including the largest moon in the solar system, Ganymede.",
    "tokens_prompt": 19,
    "tokens_completion": 292,
    "duration_s": 13.856,
    "response_time_s": 13.856,
    "energy_wh_raw": 0.129479,
    "energy_wh_net": 0.129479,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q3",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The name of the largest planet (Jupiter).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short factual recall requiring little reasoning.",
    "tags": [
      "astronomy",
      "factual",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.416331,
    "energy_kwh_per_token": 4.16e-07,
    "g_co2": 0.045318
  },
  {
    "prompt": "What is the chemical symbol for water?",
    "completion": "The chemical symbol for water is **H\u2082O**, which represents a molecule consisting of **two hydrogen atoms** (H) and **one oxygen atom** (O). \n\n- **H** is the symbol for hydrogen.  \n- **O** is the symbol for oxygen.  \n- The subscript **2** indicates there are two hydrogen atoms in each water molecule.  \n\nThis formula is commonly used in chemistry to denote water as a compound. \ud83c\udf0a",
    "tokens_prompt": 18,
    "tokens_completion": 348,
    "duration_s": 16.433,
    "response_time_s": 16.433,
    "energy_wh_raw": 0.154501,
    "energy_wh_net": 0.154501,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q4",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The two\u2011letter chemical formula (H2O).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short chemical knowledge question; minimal output.",
    "tags": [
      "chemistry",
      "short-output",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.422134,
    "energy_kwh_per_token": 4.22e-07,
    "g_co2": 0.054075
  },
  {
    "prompt": "Who wrote the play \u2018Romeo and Juliet\u2019?",
    "completion": "The play **'Romeo and Juliet'** was written by **William Shakespeare**, an English poet and playwright from the late 16th century. It is one of his most famous tragedies and is considered a cornerstone of English literature. The story explores themes of love, fate, and conflict, set against the backdrop of the feud between the Montague and Capulet families in Verona, Italy.",
    "tokens_prompt": 22,
    "tokens_completion": 256,
    "duration_s": 12.2,
    "response_time_s": 12.2,
    "energy_wh_raw": 0.113717,
    "energy_wh_net": 0.113717,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q5",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The author\u2019s name (William Shakespeare).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact retrieval; low energy.",
    "tags": [
      "literature",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.409054,
    "energy_kwh_per_token": 4.09e-07,
    "g_co2": 0.039801
  },
  {
    "prompt": "In which year did humans first land on the Moon?",
    "completion": "Humans first landed on the Moon in the year **1969**. This historic event occurred on **July 20, 1969**, as part of NASA's **Apollo 11** mission. Astronauts **Neil Armstrong** and **Buzz Aldrin** became the first humans to walk on the lunar surface, while **Michael Collins** remained in orbit aboard the command module. The mission marked a major milestone in space exploration and the Space Race between the United States and the Soviet Union.",
    "tokens_prompt": 21,
    "tokens_completion": 351,
    "duration_s": 16.61,
    "response_time_s": 16.609,
    "energy_wh_raw": 0.158976,
    "energy_wh_net": 0.158976,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q6",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A four\u2011digit year (1969).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short question with numeric answer; minimal compute.",
    "tags": [
      "history",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.427355,
    "energy_kwh_per_token": 4.27e-07,
    "g_co2": 0.055642
  },
  {
    "prompt": "Which element has the atomic number 6?",
    "completion": "The element with the atomic number 6 is **carbon**. \n\n**Explanation:**  \nThe atomic number of an element corresponds to the number of protons in its nucleus. Carbon has 6 protons, making it the sixth element on the periodic table. Its symbol is **C**, and it is a fundamental element in organic chemistry and life processes. \n\n**Answer:** Carbon (C).",
    "tokens_prompt": 19,
    "tokens_completion": 355,
    "duration_s": 17.627,
    "response_time_s": 17.627,
    "energy_wh_raw": 0.174014,
    "energy_wh_net": 0.174014,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q7",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The element\u2019s name (Carbon).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Simple factual recall; low energy cost.",
    "tags": [
      "chemistry",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.465278,
    "energy_kwh_per_token": 4.65e-07,
    "g_co2": 0.060905
  },
  {
    "prompt": "What is the square root of 49?",
    "completion": "The square root of 49 is **7**. \n\nHere's the reasoning:\n\n- A square root of a number is a value that, when multiplied by itself, gives the original number.  \n- For 49, we look for a number $ x $ such that $ x \\times x = 49 $.  \n- $ 7 \\times 7 = 49 $, so **7** is a square root of 49.  \n- Additionally, $ -7 \\times -7 = 49 $, so **-7** is also a square root.  \n- However, the symbol $ \\sqrt{} $ (square root) typically refers to the **principal (non-negative)** square root.  \n- Therefore, unless the question specifies finding all square roots, the answer is **7**.\n\n**Answer:** $ \\boxed{7} $",
    "tokens_prompt": 20,
    "tokens_completion": 898,
    "duration_s": 43.048,
    "response_time_s": 43.047,
    "energy_wh_raw": 0.424897,
    "energy_wh_net": 0.424897,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q8",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Basic arithmetic with one\u2011word output.",
    "tags": [
      "math",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.462851,
    "energy_kwh_per_token": 4.63e-07,
    "g_co2": 0.148714
  },
  {
    "prompt": "How many continents are there on Earth?",
    "completion": "The number of continents on Earth is a topic with varying classifications, but the most widely accepted answer is **seven continents**. These are:\n\n1. **Africa**  \n2. **Antarctica**  \n3. **Asia**  \n4. **Europe**  \n5. **North America**  \n6. **South America**  \n7. **Australia**  \n\nHowever, there are alternative classifications depending on context or regional conventions:\n\n- **Six continents**: Some systems combine **Europe and Asia** into **Eurasia** (e.g., in older or certain educational frameworks) or treat **Australia** as part of **Oceania** (a region that includes Australia and nearby islands).  \n- **Eight continents**: A few modern classifications include **Zealandia** (a submerged continent in the South Pacific) or split **Australia** into separate landmasses.  \n\nThe **seven-continent model** is most commonly used in geography today, though the exact count can depend on the source or educational system. \ud83c\udf0d",
    "tokens_prompt": 18,
    "tokens_completion": 687,
    "duration_s": 32.109,
    "response_time_s": 32.109,
    "energy_wh_raw": 0.312185,
    "energy_wh_net": 0.312185,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q9",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A small integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Short factual query; minimal compute and output.",
    "tags": [
      "geography",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.442816,
    "energy_kwh_per_token": 4.43e-07,
    "g_co2": 0.109265
  },
  {
    "prompt": "When you mix red and blue paint, what color do you get?",
    "completion": "When you mix red and blue paint, you get **purple**. This is because red and blue are primary colors, and their combination creates a secondary color known as purple. The exact shade of purple can vary depending on the ratio of red to blue used\u2014more red will result in a redder purple, while more blue will produce a bluer purple.",
    "tokens_prompt": 24,
    "tokens_completion": 320,
    "duration_s": 475.115,
    "response_time_s": 475.115,
    "energy_wh_raw": 0.148389,
    "energy_wh_net": 0.148389,
    "provider": "ollama",
    "model": "qwen3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q10",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The resulting secondary color (purple or violet).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact with one\u2011word answer to establish baseline energy usage.",
    "tags": [
      "color-mixing",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.431363,
    "energy_kwh_per_token": 4.31e-07,
    "g_co2": 0.051936
  }
]
