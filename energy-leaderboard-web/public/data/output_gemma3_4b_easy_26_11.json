[
  {
    "prompt": "What is the capital city of Japan?",
    "completion": "The capital city of Japan is **Tokyo**. \n\nIt\u2019s a massive, vibrant metropolis and the political, economic, and cultural center of the country. \ud83d\ude0a \n\nDo you want to know anything more about Tokyo?",
    "tokens_prompt": 17,
    "tokens_completion": 46,
    "duration_s": 7.337,
    "response_time_s": 7.337,
    "energy_wh_raw": 0.028253,
    "energy_wh_net": 0.028253,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q1",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A one\u2011word answer naming Japan\u2019s capital (Tokyo).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Very short prompt and answer provide a baseline for minimal energy consumption.",
    "tags": [
      "energy-baseline",
      "short-context",
      "factual"
    ],
    "wh_per_1k_tokens": 0.44846,
    "energy_kwh_per_token": 4.48e-07,
    "g_co2": 0.009889
  },
  {
    "prompt": "Calculate 7 + 5.",
    "completion": "7 + 5 = 12\n",
    "tokens_prompt": 16,
    "tokens_completion": 10,
    "duration_s": 0.522,
    "response_time_s": 0.522,
    "energy_wh_raw": 0.001334,
    "energy_wh_net": 0.001334,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q2",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single number representing the sum (12).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Simple arithmetic with minimal computation and output length.",
    "tags": [
      "numeric",
      "low-compute",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.051308,
    "energy_kwh_per_token": 5.1e-08,
    "g_co2": 0.000467
  },
  {
    "prompt": "Name the largest planet in our solar system.",
    "completion": "The largest planet in our solar system is **Jupiter**. \n\nIt\u2019s a gas giant and is significantly larger than all the other planets!",
    "tokens_prompt": 18,
    "tokens_completion": 30,
    "duration_s": 1.068,
    "response_time_s": 1.068,
    "energy_wh_raw": 0.006569,
    "energy_wh_net": 0.006569,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q3",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The name of the largest planet (Jupiter).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short factual recall requiring little reasoning.",
    "tags": [
      "astronomy",
      "factual",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.136854,
    "energy_kwh_per_token": 1.37e-07,
    "g_co2": 0.002299
  },
  {
    "prompt": "What is the chemical symbol for water?",
    "completion": "The chemical symbol for water is **H\u2082O**. \n\n*   **H** represents hydrogen\n*   **\u2082** indicates that there are two atoms of hydrogen\n*   **O** represents oxygen",
    "tokens_prompt": 17,
    "tokens_completion": 43,
    "duration_s": 1.409,
    "response_time_s": 1.409,
    "energy_wh_raw": 0.009725,
    "energy_wh_net": 0.009725,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q4",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The two\u2011letter chemical formula (H2O).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short chemical knowledge question; minimal output.",
    "tags": [
      "chemistry",
      "short-output",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.162083,
    "energy_kwh_per_token": 1.62e-07,
    "g_co2": 0.003404
  },
  {
    "prompt": "Who wrote the play \u2018Romeo and Juliet\u2019?",
    "completion": "William Shakespeare wrote the play \u201cRomeo and Juliet.\u201d \n\nIt\u2019s one of his most famous and beloved tragedies! \n\nDo you want to know anything more about the play or Shakespeare?",
    "tokens_prompt": 18,
    "tokens_completion": 40,
    "duration_s": 1.836,
    "response_time_s": 1.836,
    "energy_wh_raw": 0.015779,
    "energy_wh_net": 0.015779,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q5",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The author\u2019s name (William Shakespeare).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact retrieval; low energy.",
    "tags": [
      "literature",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.272052,
    "energy_kwh_per_token": 2.72e-07,
    "g_co2": 0.005523
  },
  {
    "prompt": "In which year did humans first land on the Moon?",
    "completion": "Humans first landed on the Moon in **1969**. \n\nSpecifically, it was on July 20, 1969, during the Apollo 11 mission. \n\nDo you want to know more about that mission or the history of lunar exploration?",
    "tokens_prompt": 20,
    "tokens_completion": 58,
    "duration_s": 1.819,
    "response_time_s": 1.819,
    "energy_wh_raw": 0.013303,
    "energy_wh_net": 0.013303,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q6",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A four\u2011digit year (1969).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short question with numeric answer; minimal compute.",
    "tags": [
      "history",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.170551,
    "energy_kwh_per_token": 1.71e-07,
    "g_co2": 0.004656
  },
  {
    "prompt": "Which element has the atomic number 6?",
    "completion": "The element with an atomic number of 6 is **Carbon (C)**. \n\nHere's why:\n\n*   **Atomic number** represents the number of protons in an atom's nucleus.\n*   Carbon has 6 protons.",
    "tokens_prompt": 18,
    "tokens_completion": 52,
    "duration_s": 1.684,
    "response_time_s": 1.684,
    "energy_wh_raw": 0.013124,
    "energy_wh_net": 0.013124,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q7",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The element\u2019s name (Carbon).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Simple factual recall; low energy cost.",
    "tags": [
      "chemistry",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.187486,
    "energy_kwh_per_token": 1.87e-07,
    "g_co2": 0.004593
  },
  {
    "prompt": "What is the square root of 49?",
    "completion": "The square root of 49 is 7.\n\nThis is because 7 * 7 = 49.\n",
    "tokens_prompt": 19,
    "tokens_completion": 27,
    "duration_s": 1.02,
    "response_time_s": 1.02,
    "energy_wh_raw": 0.006408,
    "energy_wh_net": 0.006408,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q8",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Basic arithmetic with one\u2011word output.",
    "tags": [
      "math",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.139304,
    "energy_kwh_per_token": 1.39e-07,
    "g_co2": 0.002243
  },
  {
    "prompt": "How many continents are there on Earth?",
    "completion": "There are **seven** continents on Earth:\n\n1.  **Africa**\n2.  **Antarctica**\n3.  **Asia**\n4.  **Australia**\n5.  **Europe**\n6.  **North America**\n7.  **South America**\n\n\nDo you want to know anything more about a specific continent?",
    "tokens_prompt": 17,
    "tokens_completion": 75,
    "duration_s": 2.273,
    "response_time_s": 2.273,
    "energy_wh_raw": 0.018427,
    "energy_wh_net": 0.018427,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q9",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A small integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Short factual query; minimal compute and output.",
    "tags": [
      "geography",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.200293,
    "energy_kwh_per_token": 2e-07,
    "g_co2": 0.006449
  },
  {
    "prompt": "When you mix red and blue paint, what color do you get?",
    "completion": "When you mix red and blue paint, you get **purple** (or violet)! \n\nIt\u2019s a classic primary color mixing example. \ud83d\ude0a \n\nDo you want to know about mixing other colors?",
    "tokens_prompt": 23,
    "tokens_completion": 43,
    "duration_s": 1.426,
    "response_time_s": 1.426,
    "energy_wh_raw": 0.009852,
    "energy_wh_net": 0.009852,
    "provider": "ollama",
    "model": "gemma3:4b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q10",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The resulting secondary color (purple or violet).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact with one\u2011word answer to establish baseline energy usage.",
    "tags": [
      "color-mixing",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.149273,
    "energy_kwh_per_token": 1.49e-07,
    "g_co2": 0.003448
  }
]
