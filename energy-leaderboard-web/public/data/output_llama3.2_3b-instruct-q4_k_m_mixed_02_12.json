[
  {
    "prompt": "Summarize the following paragraph in two sentences: Artificial intelligence is a field that has seen rapid growth in the past decade. With the advent of deep learning, machines can now perform tasks such as image recognition, language translation, and even creative writing.",
    "completion": "Here is a summary of the paragraph in two sentences:\n\nArtificial intelligence has experienced significant growth over the past decade. The emergence of deep learning technology has enabled AI systems to perform complex tasks like image recognition, language translation, and creative writing.",
    "tokens_prompt": 75,
    "tokens_completion": 49,
    "duration_s": 1.485,
    "response_time_s": 1.485,
    "energy_wh_raw": 0.011844,
    "energy_wh_net": 0.011844,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q1",
    "question_difficulty": "mixed",
    "question_task_type": "summarization",
    "expected_answer_description": "Condenses the paragraph to two sentences mentioning AI\u2019s rapid growth and its applications like image recognition, translation and creative tasks.",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Moderate summarization; tests energy usage for condensing information.",
    "tags": [
      "summarization",
      "mixed",
      "medium-output"
    ],
    "wh_per_1k_tokens": 0.095516,
    "energy_kwh_per_token": 9.6e-08,
    "g_co2": 0.004145
  },
  {
    "prompt": "Generate a short paragraph (50\u201370 words) describing a fictional planet with two suns.",
    "completion": "The planet of Aethereia is a marvel of celestial wonder, with its dual suns casting an otherworldly glow across the landscape. The binary suns, known as Solara and Helios, dance in tandem, their combined light illuminating vast expanses of iridescent cloudscapes and sapphire-hued oceans. This unique astronomical configuration has given rise to a diverse range of flora and fauna, adapted to thrive in Aethereia's dynamic, dual-sun environment.",
    "tokens_prompt": 44,
    "tokens_completion": 101,
    "duration_s": 2.453,
    "response_time_s": 2.453,
    "energy_wh_raw": 0.021767,
    "energy_wh_net": 0.021767,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q2",
    "question_difficulty": "mixed",
    "question_task_type": "reasoning",
    "expected_answer_description": "Creates a descriptive paragraph about a made\u2011up planet orbiting two stars, mentioning environmental features such as double shadows, long days or unique ecosystems.",
    "max_output_tokens_hint": 100,
    "energy_relevance": "Creative generation with word\u2011count constraint; longer output increases energy.",
    "tags": [
      "creative-writing",
      "description",
      "long-output"
    ],
    "wh_per_1k_tokens": 0.150117,
    "energy_kwh_per_token": 1.5e-07,
    "g_co2": 0.007618
  },
  {
    "prompt": "Write a JSON object with keys \u2018name\u2019, \u2018age\u2019 and \u2018favorite_color\u2019 representing a fictional character of your choice.",
    "completion": "Here's an example JSON object:\n\n```\n{\n    \"name\": \"Luna Nightshade\",\n    \"age\": 25,\n    \"favorite_color\": \"Midnight Blue\"\n}\n```\n\nThis JSON object represents Luna Nightshade, a fictional character with the specified attributes. The keys 'name', 'age' and 'favorite_color' correspond to the values provided in the JSON structure.",
    "tokens_prompt": 50,
    "tokens_completion": 80,
    "duration_s": 2.008,
    "response_time_s": 2.008,
    "energy_wh_raw": 0.016776,
    "energy_wh_net": 0.016776,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q3",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Returns a valid JSON object with three key\u2013value pairs, e.g., {\"name\": \"Alex\", \"age\": 28, \"favorite_color\": \"green\"}.",
    "max_output_tokens_hint": 30,
    "energy_relevance": "Structured output generation; tests the model\u2019s ability to follow formatting instructions.",
    "tags": [
      "json",
      "structured-output",
      "short"
    ],
    "wh_per_1k_tokens": 0.129046,
    "energy_kwh_per_token": 1.29e-07,
    "g_co2": 0.005872
  },
  {
    "prompt": "Classify the following items as \u2018fruit\u2019 or \u2018vegetable\u2019: apple, carrot, banana, broccoli, tomato. Return your answer as a list of pairs [item, type].",
    "completion": "Here is the list of classifications:\n\n- Apple, fruit\n- Carrot, vegetable\n- Banana, fruit\n- Broccoli, vegetable\n- Tomato, fruit",
    "tokens_prompt": 62,
    "tokens_completion": 34,
    "duration_s": 1.071,
    "response_time_s": 1.071,
    "energy_wh_raw": 0.00837,
    "energy_wh_net": 0.00837,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q4",
    "question_difficulty": "mixed",
    "question_task_type": "classification",
    "expected_answer_description": "Produces a list of pairs indicating fruit or vegetable classifications, e.g., [[\"apple\", \"fruit\"], \u2026].",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Requires producing a structured list and mapping categories; moderate reasoning and output length.",
    "tags": [
      "classification",
      "list",
      "structured-output"
    ],
    "wh_per_1k_tokens": 0.087188,
    "energy_kwh_per_token": 8.7e-08,
    "g_co2": 0.00293
  },
  {
    "prompt": "Translate this recipe step into Spanish: \u2018Chop the onions finely and saut\u00e9 them in olive oil until golden brown.\u2019",
    "completion": "Here's the translation:\n\n\"Picar las cebollas finamente y saltearlas en aceite de oliva hasta que est\u00e9n doradas.\"\n\nHowever, a more common way to express this in Spanish would be:\n\n\"Dorar las cebollas picadas en aceite de oliva hasta que est\u00e9n doradas.\"\n\nBut if you want to make it sound like the original English recipe, you could use:\n\n\"Chop the onions finely and saut\u00e9 them in olive oil until they're golden brown.\"\n\nNote that \"dorado\" means \"golden\", whereas \"dorar\" is a verb meaning \"to cook\". In this case, I used \"dorado\" to make it sound like the original recipe.\n\nAlternatively, you could also use:\n\n\"Chop the onions finely and saut\u00e9 them in olive oil until they're golden.\"\n\nThis translation keeps the same verb conjugation as the original English sentence.",
    "tokens_prompt": 50,
    "tokens_completion": 193,
    "duration_s": 4.493,
    "response_time_s": 4.493,
    "energy_wh_raw": 0.04083,
    "energy_wh_net": 0.04083,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q5",
    "question_difficulty": "mixed",
    "question_task_type": "translation",
    "expected_answer_description": "Provides a grammatically correct Spanish translation of the cooking instruction.",
    "max_output_tokens_hint": 40,
    "energy_relevance": "Short translation task; tests cross\u2011lingual capability with moderate compute.",
    "tags": [
      "translation",
      "cooking",
      "short"
    ],
    "wh_per_1k_tokens": 0.168025,
    "energy_kwh_per_token": 1.68e-07,
    "g_co2": 0.01429
  },
  {
    "prompt": "Given the numbers [4, 9, 16, 25], write a Python list comprehension to compute their square roots.",
    "completion": "You can use the following Python list comprehension to calculate the square root of each number in the given list:\n\n```python\nimport math\n\nnumbers = [4, 9, 16, 25]\nsquare_roots = [math.sqrt(num) for num in numbers]\n\nprint(square_roots)\n```\n\nThis will output: `[2.0, 3.0, 4.0, 5.0]`",
    "tokens_prompt": 51,
    "tokens_completion": 88,
    "duration_s": 2.21,
    "response_time_s": 2.21,
    "energy_wh_raw": 0.01869,
    "energy_wh_net": 0.01869,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q6",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Returns a one\u2011line Python list comprehension that applies math.sqrt or exponent 0.5 to each number.",
    "max_output_tokens_hint": 50,
    "energy_relevance": "Generates code; tests ability to produce correct syntax and moderate output.",
    "tags": [
      "python",
      "list-comprehension",
      "code"
    ],
    "wh_per_1k_tokens": 0.13446,
    "energy_kwh_per_token": 1.34e-07,
    "g_co2": 0.006541
  },
  {
    "prompt": "Write a haiku (three lines with syllable pattern 5\u20137\u20135) about the changing seasons.",
    "completion": "Golden leaves descend\nWinter's chill begins to bite\nSpring's warm light revives",
    "tokens_prompt": 48,
    "tokens_completion": 18,
    "duration_s": 0.688,
    "response_time_s": 0.688,
    "energy_wh_raw": 0.004567,
    "energy_wh_net": 0.004567,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q7",
    "question_difficulty": "mixed",
    "question_task_type": "other",
    "expected_answer_description": "Produces a three\u2011line haiku observing the 5\u20117\u20115 syllable structure and referencing seasonal change.",
    "max_output_tokens_hint": 40,
    "energy_relevance": "Creative poetry requires the model to track syllable counts; moderate compute despite short length.",
    "tags": [
      "poetry",
      "creative",
      "short"
    ],
    "wh_per_1k_tokens": 0.069197,
    "energy_kwh_per_token": 6.9e-08,
    "g_co2": 0.001598
  },
  {
    "prompt": "State the moral of Aesop\u2019s fable \u2018The Tortoise and the Hare\u2019 in one sentence.",
    "completion": "The moral of Aesop's fable 'The Tortoise and the Hare' is that slow and steady effort can be more successful than relying on natural talent or speed, and that perseverance and determination are often more important than innate abilities.",
    "tokens_prompt": 48,
    "tokens_completion": 50,
    "duration_s": 1.378,
    "response_time_s": 1.378,
    "energy_wh_raw": 0.010939,
    "energy_wh_net": 0.010939,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q8",
    "question_difficulty": "mixed",
    "question_task_type": "summarization",
    "expected_answer_description": "Summarizes the moral (steady effort and perseverance often outperform careless speed).",
    "max_output_tokens_hint": 25,
    "energy_relevance": "Short summarization; tests ability to capture key lesson succinctly.",
    "tags": [
      "fable",
      "moral",
      "short"
    ],
    "wh_per_1k_tokens": 0.111622,
    "energy_kwh_per_token": 1.12e-07,
    "g_co2": 0.003829
  },
  {
    "prompt": "Explain what a binary search algorithm does in two sentences.",
    "completion": "A binary search algorithm is an efficient method for finding a specific element within a sorted list by repeatedly dividing the list in half and searching for the target value in one of the resulting sublists. This process continues until the target value is found, with each iteration reducing the search space by half, making it a highly effective approach for large datasets.",
    "tokens_prompt": 37,
    "tokens_completion": 69,
    "duration_s": 1.742,
    "response_time_s": 1.742,
    "energy_wh_raw": 0.015046,
    "energy_wh_net": 0.015046,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q9",
    "question_difficulty": "mixed",
    "question_task_type": "reasoning",
    "expected_answer_description": "Describes that binary search repeatedly divides a sorted list to locate a target value by comparing the target to the middle element and narrowing the search interval.",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Algorithm description requires concise reasoning; moderate output length.",
    "tags": [
      "computer-science",
      "algorithm",
      "medium-output"
    ],
    "wh_per_1k_tokens": 0.141943,
    "energy_kwh_per_token": 1.42e-07,
    "g_co2": 0.005266
  },
  {
    "prompt": "Given the string \u2018hello world\u2019, write pseudocode or simple code to reverse the string and briefly explain your approach.",
    "completion": "# Reversing a String in Pseudocode\n\nHere is a simple pseudocode to reverse a given string:\n\n```\nFUNCTION reverse_string(input_string)\n  output_string = empty string\n  FOR i FROM length of input_string - 1 TO 0\n    output_string = append character at index i to output_string\n  RETURN output_string\nEND FUNCTION\n```\n\nIn this pseudocode, we start from the last character of the input string and move towards the beginning. We use a for loop that starts from `length of input_string - 1` (the last character) and ends at `0`. In each iteration, we append the current character to our output string.\n\nFor example, if we have the string 'hello world', here's how it would be reversed:\n\n```\ninput_string: 'hello world'\nlength: 11\n\nIteration 1:\ni = 10 (last character)\noutput_string: ''\nappend 'd' to output_string\noutput_string: 'd'\n\nIteration 2:\ni = 9 ('orl' at index 9)\noutput_string: 'd orl'\nappend 'o' to output_string\noutput_string: 'd orlo'\n\nIteration 3:\ni = 8 ('llow' at index 8)\noutput_string: 'd orlo llow'\nappend 'w' to output_string\noutput_string: 'd orlow'\n\nIteration 4:\ni = 7 ('ello' at index 7)\noutput_string: 'd orlow ello'\nappend 'h' to output_string\noutput_string: 'd orlow ellho'\n\nIteration 5:\ni = 6 ('he' at index 6)\noutput_string: 'd orlow ellho he'\nappend 'l' to output_string\noutput_string: 'd orlow ellhol'\n\nIteration 6:\ni = 5 ('lo' at index 5)\noutput_string: 'd orlow ellhol lo'\nappend 'w' to output_string\noutput_string: 'd orlow ellhol w'\n\nIteration 7:\ni = 4 ('ll' at index 4)\noutput_string: 'd orlow ellhol w ll'\nappend 'o' to output_string\noutput_string: 'd orlow ellhol wo\n\nIteration 8:\ni = 3 ('wo' at index 3)\noutput_string: 'd orlow ellhol wo wo'\nappend 'r' to output_string\noutput_string: 'd orlow ellhol wor'\n\nIteration 9:\ni = 2 ('or' at index 2)\noutput_string: 'd orlow ellhol wor or'\nappend 'l' to output_string\noutput_string: 'd orlow ellhol worr\n\nIteration 10:\ni = 1 ('llow' at index 1)\noutput_string: 'd orlow ellhol worr llow'\nappend 'e' to output_string\noutput_string: 'd orlow ellhol worr le'\n\nIteration 11:\ni = 0 (first character 'h')\noutput_string: 'd orlow ellhol worr le h'\nappend 'l' to output_string\noutput_string: 'd orlow ellhol worr lhe\n\noutput_string: 'dlrow olleH'\n```\n\nTherefore, the reversed string is 'dlrow olleH'.",
    "tokens_prompt": 48,
    "tokens_completion": 691,
    "duration_s": 16.042,
    "response_time_s": 16.042,
    "energy_wh_raw": 0.149157,
    "energy_wh_net": 0.149157,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with Quadro T2000 with Max-Q Design",
    "device_type": "nvidia",
    "os_name": "Linux",
    "os_version": "5.15.167.4-microsoft-standard-WSL2",
    "cpu_model": "Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz",
    "gpu_model": "Quadro T2000 with Max-Q Design",
    "ram_gb": 31.2,
    "chip_architecture": "x86_64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q10",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Provides pseudocode or a short function that iterates over the characters or uses slicing to reverse the string and explains the method.",
    "max_output_tokens_hint": 80,
    "energy_relevance": "Combines code and explanation; increases output length and tests multi\u2011modal reasoning.",
    "tags": [
      "coding",
      "string-manipulation",
      "explanation"
    ],
    "wh_per_1k_tokens": 0.201836,
    "energy_kwh_per_token": 2.02e-07,
    "g_co2": 0.052205
  }
]