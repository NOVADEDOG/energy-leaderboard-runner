[
  {
    "prompt": "Summarize the following paragraph in two sentences: Artificial intelligence is a field that has seen rapid growth in the past decade. With the advent of deep learning, machines can now perform tasks such as image recognition, language translation, and even creative writing.",
    "completion": "Here is a summary of the paragraph in two sentences:\n\nArtificial intelligence (AI) has experienced significant growth over the past decade, driven by advancements in machine learning technologies. Recent breakthroughs in deep learning have enabled machines to perform complex tasks such as image recognition, language translation, and even creative writing.",
    "tokens_prompt": 75,
    "tokens_completion": 61,
    "duration_s": 1.913,
    "response_time_s": 1.913,
    "energy_wh_raw": 0.014047,
    "energy_wh_net": 0.014047,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q1",
    "question_difficulty": "mixed",
    "question_task_type": "summarization",
    "expected_answer_description": "Condenses the paragraph to two sentences mentioning AI\u2019s rapid growth and its applications like image recognition, translation and creative tasks.",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Moderate summarization; tests energy usage for condensing information.",
    "tags": [
      "summarization",
      "mixed",
      "medium-output"
    ],
    "wh_per_1k_tokens": 0.103287,
    "energy_kwh_per_token": 1.03e-07,
    "g_co2": 0.004916
  },
  {
    "prompt": "Generate a short paragraph (50\u201370 words) describing a fictional planet with two suns.",
    "completion": "The dual-sun planet of Aethereia is a celestial wonder, where twin orbs of light dance across the horizon. The planet's dichotomous landscape reflects this duality, with scorching deserts mirroring the fiery passion of one sun and lush forests reflecting the soothing tranquility of the other. As day shifts to night, Aethereia's inhabitants adapt to a rhythm of eternal twilight, weaving their lives amidst the perpetual, golden glow.",
    "tokens_prompt": 44,
    "tokens_completion": 95,
    "duration_s": 2.647,
    "response_time_s": 2.647,
    "energy_wh_raw": 0.019658,
    "energy_wh_net": 0.019658,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q2",
    "question_difficulty": "mixed",
    "question_task_type": "reasoning",
    "expected_answer_description": "Creates a descriptive paragraph about a made\u2011up planet orbiting two stars, mentioning environmental features such as double shadows, long days or unique ecosystems.",
    "max_output_tokens_hint": 100,
    "energy_relevance": "Creative generation with word\u2011count constraint; longer output increases energy.",
    "tags": [
      "creative-writing",
      "description",
      "long-output"
    ],
    "wh_per_1k_tokens": 0.141424,
    "energy_kwh_per_token": 1.41e-07,
    "g_co2": 0.00688
  },
  {
    "prompt": "Write a JSON object with keys \u2018name\u2019, \u2018age\u2019 and \u2018favorite_color\u2019 representing a fictional character of your choice.",
    "completion": "Here is a JSON object representing a fictional character:\n\n```\n{\n    \"name\": \"Eryndor Thorne\",\n    \"age\": 25,\n    \"favorite_color\": \"Midnight Blue\"\n}\n```",
    "tokens_prompt": 50,
    "tokens_completion": 45,
    "duration_s": 1.364,
    "response_time_s": 1.364,
    "energy_wh_raw": 0.008912,
    "energy_wh_net": 0.008912,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q3",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Returns a valid JSON object with three key\u2013value pairs, e.g., {\"name\": \"Alex\", \"age\": 28, \"favorite_color\": \"green\"}.",
    "max_output_tokens_hint": 30,
    "energy_relevance": "Structured output generation; tests the model\u2019s ability to follow formatting instructions.",
    "tags": [
      "json",
      "structured-output",
      "short"
    ],
    "wh_per_1k_tokens": 0.093811,
    "energy_kwh_per_token": 9.4e-08,
    "g_co2": 0.003119
  },
  {
    "prompt": "Classify the following items as \u2018fruit\u2019 or \u2018vegetable\u2019: apple, carrot, banana, broccoli, tomato. Return your answer as a list of pairs [item, type].",
    "completion": "Here is the classification:\n\n* Apple, fruit\n* Carrot, vegetable\n* Banana, fruit\n* Broccoli, vegetable\n* Tomato, vegetable",
    "tokens_prompt": 62,
    "tokens_completion": 32,
    "duration_s": 1.07,
    "response_time_s": 1.07,
    "energy_wh_raw": 0.007411,
    "energy_wh_net": 0.007411,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q4",
    "question_difficulty": "mixed",
    "question_task_type": "classification",
    "expected_answer_description": "Produces a list of pairs indicating fruit or vegetable classifications, e.g., [[\"apple\", \"fruit\"], \u2026].",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Requires producing a structured list and mapping categories; moderate reasoning and output length.",
    "tags": [
      "classification",
      "list",
      "structured-output"
    ],
    "wh_per_1k_tokens": 0.07884,
    "energy_kwh_per_token": 7.9e-08,
    "g_co2": 0.002594
  },
  {
    "prompt": "Translate this recipe step into Spanish: \u2018Chop the onions finely and saut\u00e9 them in olive oil until golden brown.\u2019",
    "completion": "Here's the translation:\n\n\"Mezcla los cebollos finamente y saltea en aceite de oliva hasta que est\u00e9n dorados.\"\n\nNote: I used the word \"cebollos\" instead of just \"cebolla\" to maintain a more formal tone, as it's a plural noun.\n\nAlternatively, you could also use:\n\n\"Sacuda los cebollos finamente y saltea en aceite de oliva hasta que est\u00e9n dorados.\"\n\nIn this version, I used the verb \"sacudir\", which means \"to chop\" or \"to mince\", but it's often used in a more general sense to describe chopping or cutting something finely.",
    "tokens_prompt": 50,
    "tokens_completion": 142,
    "duration_s": 3.903,
    "response_time_s": 3.903,
    "energy_wh_raw": 0.030138,
    "energy_wh_net": 0.030138,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q5",
    "question_difficulty": "mixed",
    "question_task_type": "translation",
    "expected_answer_description": "Provides a grammatically correct Spanish translation of the cooking instruction.",
    "max_output_tokens_hint": 40,
    "energy_relevance": "Short translation task; tests cross\u2011lingual capability with moderate compute.",
    "tags": [
      "translation",
      "cooking",
      "short"
    ],
    "wh_per_1k_tokens": 0.156969,
    "energy_kwh_per_token": 1.57e-07,
    "g_co2": 0.010548
  },
  {
    "prompt": "Given the numbers [4, 9, 16, 25], write a Python list comprehension to compute their square roots.",
    "completion": "You can use the `math.sqrt()` function from Python's math module along with list comprehension to calculate the square root of each number in the given list.\n\nHere is an example:\n\n```python\nimport math\n\nnumbers = [4, 9, 16, 25]\nsquare_roots = [math.sqrt(n) for n in numbers]\n\nprint(square_roots)\n```\n\nThis code will output: `[2.0, 3.0, 4.0, 5.0]`\n\nNote that `math.sqrt()` returns the square root of a number exactly if possible. For example, `math.sqrt(16)` returns `4.0` instead of `3.9999999999999996`.",
    "tokens_prompt": 51,
    "tokens_completion": 148,
    "duration_s": 4.033,
    "response_time_s": 4.033,
    "energy_wh_raw": 0.030937,
    "energy_wh_net": 0.030937,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q6",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Returns a one\u2011line Python list comprehension that applies math.sqrt or exponent 0.5 to each number.",
    "max_output_tokens_hint": 50,
    "energy_relevance": "Generates code; tests ability to produce correct syntax and moderate output.",
    "tags": [
      "python",
      "list-comprehension",
      "code"
    ],
    "wh_per_1k_tokens": 0.155462,
    "energy_kwh_per_token": 1.55e-07,
    "g_co2": 0.010828
  },
  {
    "prompt": "Write a haiku (three lines with syllable pattern 5\u20137\u20135) about the changing seasons.",
    "completion": "Golden leaves fall slow\nSummer's warmth gives way to cold\nWinter's snowy hush",
    "tokens_prompt": 48,
    "tokens_completion": 19,
    "duration_s": 0.66,
    "response_time_s": 0.66,
    "energy_wh_raw": 0.003087,
    "energy_wh_net": 0.003087,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q7",
    "question_difficulty": "mixed",
    "question_task_type": "other",
    "expected_answer_description": "Produces a three\u2011line haiku observing the 5\u20117\u20115 syllable structure and referencing seasonal change.",
    "max_output_tokens_hint": 40,
    "energy_relevance": "Creative poetry requires the model to track syllable counts; moderate compute despite short length.",
    "tags": [
      "poetry",
      "creative",
      "short"
    ],
    "wh_per_1k_tokens": 0.046075,
    "energy_kwh_per_token": 4.6e-08,
    "g_co2": 0.00108
  },
  {
    "prompt": "State the moral of Aesop\u2019s fable \u2018The Tortoise and the Hare\u2019 in one sentence.",
    "completion": "The moral of 'The Tortoise and the Hare' is that slow and steady effort can often lead to greater success than relying on natural talent or speed, as hard work and perseverance can overcome initial advantages.",
    "tokens_prompt": 48,
    "tokens_completion": 43,
    "duration_s": 1.282,
    "response_time_s": 1.282,
    "energy_wh_raw": 0.008075,
    "energy_wh_net": 0.008075,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q8",
    "question_difficulty": "mixed",
    "question_task_type": "summarization",
    "expected_answer_description": "Summarizes the moral (steady effort and perseverance often outperform careless speed).",
    "max_output_tokens_hint": 25,
    "energy_relevance": "Short summarization; tests ability to capture key lesson succinctly.",
    "tags": [
      "fable",
      "moral",
      "short"
    ],
    "wh_per_1k_tokens": 0.088736,
    "energy_kwh_per_token": 8.9e-08,
    "g_co2": 0.002826
  },
  {
    "prompt": "Explain what a binary search algorithm does in two sentences.",
    "completion": "A binary search algorithm is an efficient method for finding a specific value within a sorted array by repeatedly dividing the array in half and searching for the target value in one of the two halves until it's found or it's determined that the value isn't present. This process reduces the number of comparisons needed to find the target value, making it much faster than linear search algorithms, especially for large datasets.",
    "tokens_prompt": 37,
    "tokens_completion": 80,
    "duration_s": 2.227,
    "response_time_s": 2.227,
    "energy_wh_raw": 0.016006,
    "energy_wh_net": 0.016006,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q9",
    "question_difficulty": "mixed",
    "question_task_type": "reasoning",
    "expected_answer_description": "Describes that binary search repeatedly divides a sorted list to locate a target value by comparing the target to the middle element and narrowing the search interval.",
    "max_output_tokens_hint": 60,
    "energy_relevance": "Algorithm description requires concise reasoning; moderate output length.",
    "tags": [
      "computer-science",
      "algorithm",
      "medium-output"
    ],
    "wh_per_1k_tokens": 0.136803,
    "energy_kwh_per_token": 1.37e-07,
    "g_co2": 0.005602
  },
  {
    "prompt": "Given the string \u2018hello world\u2019, write pseudocode or simple code to reverse the string and briefly explain your approach.",
    "completion": "def reverse_string(input_str):\n    reversed_str = \"\"\n    for char in input_str:\n        reversed_str = char + reversed_str\n    return reversed_str\n\n# Example usage:\ninput_str = \"hello world\"\nreversed_str = reverse_string(input_str)\nprint(reversed_str)  # Output: \"dlrow olleh\"",
    "tokens_prompt": 48,
    "tokens_completion": 68,
    "duration_s": 1.94,
    "response_time_s": 1.94,
    "energy_wh_raw": 0.013682,
    "energy_wh_net": 0.013682,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts4",
    "testset_name": "Mixed task suite (realistic workload)",
    "testset_goal": "Provide a mixture of reasoning, summarization, classification, translation and code tasks to emulate a real\u2011world workload and observe how different operations affect energy consumption.",
    "testset_notes": "This set mixes short and long prompts, structured outputs and creative tasks. Use it to test energy consumption under varied workloads. Track tokens per joule and compare models across task types, as recommended by research on tokens/J metrics.",
    "question_id": "ts4_q10",
    "question_difficulty": "mixed",
    "question_task_type": "coding",
    "expected_answer_description": "Provides pseudocode or a short function that iterates over the characters or uses slicing to reverse the string and explains the method.",
    "max_output_tokens_hint": 80,
    "energy_relevance": "Combines code and explanation; increases output length and tests multi\u2011modal reasoning.",
    "tags": [
      "coding",
      "string-manipulation",
      "explanation"
    ],
    "wh_per_1k_tokens": 0.117948,
    "energy_kwh_per_token": 1.18e-07,
    "g_co2": 0.004789
  }
]