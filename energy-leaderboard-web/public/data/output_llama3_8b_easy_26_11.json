[
  {
    "prompt": "What is the capital city of Japan?",
    "completion": "The capital city of Japan is Tokyo.",
    "tokens_prompt": 18,
    "tokens_completion": 9,
    "duration_s": 5.677,
    "response_time_s": 5.677,
    "energy_wh_raw": 0.014512,
    "energy_wh_net": 0.014512,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q1",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A one\u2011word answer naming Japan\u2019s capital (Tokyo).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Very short prompt and answer provide a baseline for minimal energy consumption.",
    "tags": [
      "energy-baseline",
      "short-context",
      "factual"
    ],
    "wh_per_1k_tokens": 0.537481,
    "energy_kwh_per_token": 5.37e-07,
    "g_co2": 0.005079
  },
  {
    "prompt": "Calculate 7 + 5.",
    "completion": "The answer is:\n\n7 + 5 = 12",
    "tokens_prompt": 17,
    "tokens_completion": 12,
    "duration_s": 0.667,
    "response_time_s": 0.667,
    "energy_wh_raw": 0.0019,
    "energy_wh_net": 0.0019,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q2",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single number representing the sum (12).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Simple arithmetic with minimal computation and output length.",
    "tags": [
      "numeric",
      "low-compute",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.065517,
    "energy_kwh_per_token": 6.6e-08,
    "g_co2": 0.000665
  },
  {
    "prompt": "Name the largest planet in our solar system.",
    "completion": "The largest planet in our solar system is Jupiter! It's a gas giant, with a diameter of approximately 142,984 kilometers (88,846 miles).",
    "tokens_prompt": 19,
    "tokens_completion": 33,
    "duration_s": 1.403,
    "response_time_s": 1.403,
    "energy_wh_raw": 0.008841,
    "energy_wh_net": 0.008841,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q3",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The name of the largest planet (Jupiter).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short factual recall requiring little reasoning.",
    "tags": [
      "astronomy",
      "factual",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.170019,
    "energy_kwh_per_token": 1.7e-07,
    "g_co2": 0.003094
  },
  {
    "prompt": "What is the chemical symbol for water?",
    "completion": "The chemical symbol for water is H2O. It consists of two hydrogen atoms (H) and one oxygen atom (O).",
    "tokens_prompt": 18,
    "tokens_completion": 27,
    "duration_s": 1.189,
    "response_time_s": 1.189,
    "energy_wh_raw": 0.006912,
    "energy_wh_net": 0.006912,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q4",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The two\u2011letter chemical formula (H2O).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short chemical knowledge question; minimal output.",
    "tags": [
      "chemistry",
      "short-output",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.1536,
    "energy_kwh_per_token": 1.54e-07,
    "g_co2": 0.002419
  },
  {
    "prompt": "Who wrote the play \u2018Romeo and Juliet\u2019?",
    "completion": "The play \"Romeo and Juliet\" was written by William Shakespeare. It is one of his most famous plays, and it tells the story of two young lovers from feuding families who fall in love and ultimately meet a tragic end. The play is believed to have been written around 1595-1596.",
    "tokens_prompt": 22,
    "tokens_completion": 65,
    "duration_s": 2.599,
    "response_time_s": 2.599,
    "energy_wh_raw": 0.019602,
    "energy_wh_net": 0.019602,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q5",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The author\u2019s name (William Shakespeare).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact retrieval; low energy.",
    "tags": [
      "literature",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.22531,
    "energy_kwh_per_token": 2.25e-07,
    "g_co2": 0.006861
  },
  {
    "prompt": "In which year did humans first land on the Moon?",
    "completion": "Humans first landed on the Moon on July 20, 1969. The Apollo 11 mission, crewed by astronauts Neil Armstrong and Edwin \"Buzz\" Aldrin, successfully touched down on the Moon's surface in the Lunar Module Eagle. Armstrong became the first person to set foot on the Moon, famously declaring \"That's one small step for man, one giant leap for mankind\" as he stepped onto the lunar surface.",
    "tokens_prompt": 21,
    "tokens_completion": 87,
    "duration_s": 3.406,
    "response_time_s": 3.406,
    "energy_wh_raw": 0.027623,
    "energy_wh_net": 0.027623,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q6",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A four\u2011digit year (1969).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short question with numeric answer; minimal compute.",
    "tags": [
      "history",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.255769,
    "energy_kwh_per_token": 2.56e-07,
    "g_co2": 0.009668
  },
  {
    "prompt": "Which element has the atomic number 6?",
    "completion": "The element with the atomic number 6 is Carbon (C).",
    "tokens_prompt": 19,
    "tokens_completion": 14,
    "duration_s": 0.734,
    "response_time_s": 0.734,
    "energy_wh_raw": 0.003172,
    "energy_wh_net": 0.003172,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q7",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The element\u2019s name (Carbon).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Simple factual recall; low energy cost.",
    "tags": [
      "chemistry",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.096121,
    "energy_kwh_per_token": 9.6e-08,
    "g_co2": 0.00111
  },
  {
    "prompt": "What is the square root of 49?",
    "completion": "The square root of 49 is 7.",
    "tokens_prompt": 19,
    "tokens_completion": 11,
    "duration_s": 0.632,
    "response_time_s": 0.632,
    "energy_wh_raw": 0.002111,
    "energy_wh_net": 0.002111,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q8",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Basic arithmetic with one\u2011word output.",
    "tags": [
      "math",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.070367,
    "energy_kwh_per_token": 7e-08,
    "g_co2": 0.000739
  },
  {
    "prompt": "How many continents are there on Earth?",
    "completion": "A simple question!\n\nThere are 7 continents on Earth:\n\n1. Africa\n2. Antarctica\n3. Asia\n4. Australia\n5. Europe\n6. North America\n7. South America\n\nNote: Some sources may group Europe and Asia together as a single continent, Eurasia, or combine North and South America as the American continent. However, the 7-continent model is the most commonly accepted classification.",
    "tokens_prompt": 18,
    "tokens_completion": 87,
    "duration_s": 3.513,
    "response_time_s": 3.513,
    "energy_wh_raw": 0.030822,
    "energy_wh_net": 0.030822,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q9",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A small integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Short factual query; minimal compute and output.",
    "tags": [
      "geography",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.293543,
    "energy_kwh_per_token": 2.94e-07,
    "g_co2": 0.010788
  },
  {
    "prompt": "When you mix red and blue paint, what color do you get?",
    "completion": "When you mix red and blue paint, you get the color purple!",
    "tokens_prompt": 24,
    "tokens_completion": 15,
    "duration_s": 0.765,
    "response_time_s": 0.765,
    "energy_wh_raw": 0.003232,
    "energy_wh_net": 0.003232,
    "provider": "ollama",
    "model": "llama3:8b",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q10",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The resulting secondary color (purple or violet).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact with one\u2011word answer to establish baseline energy usage.",
    "tags": [
      "color-mixing",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.082872,
    "energy_kwh_per_token": 8.3e-08,
    "g_co2": 0.001131
  }
]
