[
  {
    "prompt": "What is the capital city of Japan?",
    "completion": "The user asks a simple question: \"What is the capital city of Japan?\" The answer: Tokyo. Provide straightforward response.\n</think>\nThe capital city of Japan is **Tokyo**.",
    "tokens_prompt": 24,
    "tokens_completion": 37,
    "duration_s": 1.797,
    "response_time_s": 1.797,
    "energy_wh_raw": 0.012789,
    "energy_wh_net": 0.012789,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q1",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A one\u2011word answer naming Japan\u2019s capital (Tokyo).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Very short prompt and answer provide a baseline for minimal energy consumption.",
    "tags": [
      "energy-baseline",
      "short-context",
      "factual"
    ],
    "wh_per_1k_tokens": 0.209656,
    "energy_kwh_per_token": 2.1e-07,
    "g_co2": 0.004476
  },
  {
    "prompt": "Calculate 7 + 5.",
    "completion": "User asks to calculate simple arithmetic: 7+5 =12.\n</think>\n\\(7 + 5 = 12\\)",
    "tokens_prompt": 23,
    "tokens_completion": 28,
    "duration_s": 1.416,
    "response_time_s": 1.416,
    "energy_wh_raw": 0.017063,
    "energy_wh_net": 0.017063,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q2",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single number representing the sum (12).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Simple arithmetic with minimal computation and output length.",
    "tags": [
      "numeric",
      "low-compute",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.334569,
    "energy_kwh_per_token": 3.35e-07,
    "g_co2": 0.005972
  },
  {
    "prompt": "Name the largest planet in our solar system.",
    "completion": "We need to answer: The largest planet is Jupiter. Provide name.\n</think>\nThe largest planet in our solar system is **Jupiter**. It\u2019s a gas giant with a mass more than twice that of all the other planets combined and a diameter of about 139,820\u202fkm\u2014roughly 11 times Earth\u2019s size.",
    "tokens_prompt": 25,
    "tokens_completion": 73,
    "duration_s": 3.279,
    "response_time_s": 3.279,
    "energy_wh_raw": 0.049592,
    "energy_wh_net": 0.049592,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q3",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The name of the largest planet (Jupiter).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short factual recall requiring little reasoning.",
    "tags": [
      "astronomy",
      "factual",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.506041,
    "energy_kwh_per_token": 5.06e-07,
    "g_co2": 0.017357
  },
  {
    "prompt": "What is the chemical symbol for water?",
    "completion": "The user asks: \"What is the chemical symbol for water?\" This is a simple question. Answer: The molecular formula of water is H2O, but often we refer to it as H2O; there's no single-element chemical symbol. Possibly they mean H2O? Or perhaps they think about water being made up from hydrogen and oxygen, so the symbol is H\u2082O. We can answer with explanation: chemical symbol is H2O. Provide context.\n</think>\nThe \u201cchemical symbol\u201d for a molecule of water isn\u2019t a single element symbol\u2014it\u2019s written as **H\u2082O**.  \n\n- **H**\u202f= hydrogen (two atoms)  \n- **O**\u202f= oxygen (one atom)  \n\nSo the formula **H\u2082O** tells us that each water molecule consists of two hydrogen atoms covalently bonded to one oxygen atom. If you\u2019re referring specifically to elemental components, there isn\u2019t a separate symbol for \u201cwater\u201d as an element; the molecular formula is what we use to represent it chemically.",
    "tokens_prompt": 24,
    "tokens_completion": 215,
    "duration_s": 9.133,
    "response_time_s": 9.133,
    "energy_wh_raw": 0.137605,
    "energy_wh_net": 0.137605,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q4",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The two\u2011letter chemical formula (H2O).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short chemical knowledge question; minimal output.",
    "tags": [
      "chemistry",
      "short-output",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.575753,
    "energy_kwh_per_token": 5.76e-07,
    "g_co2": 0.048162
  },
  {
    "prompt": "Who wrote the play \u2018Romeo and Juliet\u2019?",
    "completion": "User asks a straightforward question: Who wrote the play 'Romeo and Juliet'? I should answer with Shakespeare. Possibly provide extra context? The user just asked who wrote it; answer concisely.\n</think>\n\u2018Romeo\u202fand\u202fJuliet\u2019 was written by **William Shakespeare**. It is one of his most famous tragic plays, first performed in the late 16th century.",
    "tokens_prompt": 27,
    "tokens_completion": 82,
    "duration_s": 3.69,
    "response_time_s": 3.69,
    "energy_wh_raw": 0.052886,
    "energy_wh_net": 0.052886,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q5",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The author\u2019s name (William Shakespeare).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact retrieval; low energy.",
    "tags": [
      "literature",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.485193,
    "energy_kwh_per_token": 4.85e-07,
    "g_co2": 0.01851
  },
  {
    "prompt": "In which year did humans first land on the Moon?",
    "completion": "The user asks a simple factual question: \"In which year did humans first land on the Moon?\" The answer is 1969 (specifically July 20, 1969). Provide concise answer.\n</think>\nHumans first landed on the Moon in **1969**, when NASA\u2019s Apollo\u202f11 mission touched down on the lunar surface on July\u202f20 of that year.",
    "tokens_prompt": 27,
    "tokens_completion": 86,
    "duration_s": 3.852,
    "response_time_s": 3.852,
    "energy_wh_raw": 0.055366,
    "energy_wh_net": 0.055366,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q6",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A four\u2011digit year (1969).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short question with numeric answer; minimal compute.",
    "tags": [
      "history",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.489965,
    "energy_kwh_per_token": 4.9e-07,
    "g_co2": 0.019378
  },
  {
    "prompt": "Which element has the atomic number 6?",
    "completion": "Okay, the user is asking which element has an atomic number of 6. Let me recall the basics of atomic numbers. The atomic number represents the number of protons in an atom's nucleus. For neutral atoms, it also equals the number of electrons.\n\nHmm, I remember that carbon has 6 protons. That's a fundamental detail taught early in chemistry. But let me double-check to be thorough. Hydrogen is 1, helium is 2, lithium is 3, beryllium is 4, boron is 5... yes, the sixth element should indeed be carbon.\n\nThe user might be a student just starting to learn chemistry. The question seems very basic, so they're probably at an introductory level. I should keep the answer clear and simple without overwhelming them with jargon. But since it's such a foundational question, maybe they're testing if I know the basics or preparing for an exam.\n\nI wonder if there's any trick here. Could atomic number 6 refer to something else? Like in certain isotopes? No, atomic number is strictly about protons - it's fixed for each element regardless of isotope. So carbon-12 or carbon-14 both have atomic number 6. The element name remains carbon.\n\nShould I mention isotopes? Probably not necessary since the question just asks \"which element\", implying they want the standard elemental identity. Just confirming that all atoms with 6 protons are called carbon.\n\nThe user didn't specify context, but given how simple this is, maybe they're:\n- A young learner\n- Someone creating educational material for kids\n- Or testing if I'll give overcomplicated answers\n\nBest to keep it concise: \"Carbon\" followed by one sentence explanation. No need to dive into quantum numbers or electron configurations unless asked.\n</think>\nThe element with an atomic number of **6** is **carbon**.  \n\n### Key Details:\n- **Atomic number (Z)** = Number of protons in the nucleus of a neutral atom.  \n- Carbon has **6 protons**, defining it as element #6 on the periodic table.  \n- It belongs to group 14 and period 2, with an electron configuration of `1s\u00b2 2s\u00b2 2p\u00b2`.  \n\n### Why Not Others?\n- Hydrogen: Z = 1 (1 proton)  \n- Helium: Z = 2 (2 protons)  \n- Boron: Z = 5 (5 protons)  \n- **Nitrogen**: Z = 7 (7 protons) \u2192 *Common mistake; atomic number 6 is strictly carbon.*  \n\nCarbon is fundamental to organic chemistry, life on Earth (e.g., DNA, proteins), and has isotopes like carbon-12 (\u00b9\u00b2C, 98.9% natural abundance). If you're studying chemistry basics, this is a foundational fact! Let me know if you'd like deeper details. \ud83d\ude0a",
    "tokens_prompt": 25,
    "tokens_completion": 601,
    "duration_s": 25.63,
    "response_time_s": 25.63,
    "energy_wh_raw": 0.378838,
    "energy_wh_net": 0.378838,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q7",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The element\u2019s name (Carbon).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Simple factual recall; low energy cost.",
    "tags": [
      "chemistry",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.605173,
    "energy_kwh_per_token": 6.05e-07,
    "g_co2": 0.132593
  },
  {
    "prompt": "What is the square root of 49?",
    "completion": "User asks a simple math question: \"What is the square root of 49?\" Answer should be 7 (principal square root). Maybe also mention both positive and negative roots. Provide answer.\n</think>\nThe prime\u2011square number\u202f49 has two square roots:\n\n- **Positive:** \u221a49\u202f=\u202f7  \n- **Negative:** \u2013\u221a49\u202f=\u202f\u20137  \n\nIn most contexts, when we ask for \u201cthe\u201d square root of a positive number we refer to the principal (non\u2011negative) root, which is **7**.",
    "tokens_prompt": 26,
    "tokens_completion": 120,
    "duration_s": 5.298,
    "response_time_s": 5.298,
    "energy_wh_raw": 0.077366,
    "energy_wh_net": 0.077366,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q8",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Basic arithmetic with one\u2011word output.",
    "tags": [
      "math",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.529904,
    "energy_kwh_per_token": 5.3e-07,
    "g_co2": 0.027078
  },
  {
    "prompt": "How many continents are there on Earth?",
    "completion": "The user asks a simple factual question: \"How many continents are there on Earth?\" Provide answer. We can explain that commonly 7 continents, but some models use 5 or 6. But likely expects number: seven. Give explanation.\n</think>\nThere is not a single universally\u2011agreed answer\u2014different educational systems and organizations group the world\u2019s landmasses in slightly different ways\u2014but the **most widely taught model today recognizes\u202fseven distinct continents**:\n\n1. Africa  \n2. Antarctica  \n3. Asia  \n4. Europe  \n5. North America  \n6. South America  \n7. Australia (sometimes called Oceania, which can also include nearby islands)\n\n**Why the count can vary**\n\n| Model | # of Continents | Reasoning |\n|-------|-----------------|----------|\n| **Seven\u2011continent model** | 7 | Most common in English\u2011language textbooks; separates Europe and Asia despite them sitting on a single landmass (Eurasia). Often also splits North and South America. |\n| **Six\u2011continent model** | 6 | Combines Europe and Asia into \u201cEurasia\u201d (or combines the Americas into \u201cAmerica\u201d). Used in some European, Russian, Japanese curricula. |\n| **Five\u2011continent model** | 5 | Merges the Americas together and sometimes groups Africa/Europe/Asia as one supercontinent; occasionally excludes Antarctica when focusing on inhabited lands only. |\n| **Four\u2011continent model (Olympic)** | 4 | Joins Europe, Asia, and sometimes Africa into \u201cAfro\u2011Eurasia\u201d and combines North & South America; used in the context of the Olympic Games\u2019 zones. |\n\n**Key points to remember**\n\n- The number depends on cultural, educational, or political conventions rather than on any objective geological boundary.\n- **Geologically**, Earth\u2019s major landmasses can be viewed as fewer supercontinents (e.g., *Eurasia*, *America*), but the seven\u2011continent scheme remains dominant in world maps and most school curricula.\n- If you need a specific answer for an exam or assignment, check the format your teacher/author expects\u2014many standardized tests list **seven** continents.",
    "tokens_prompt": 24,
    "tokens_completion": 459,
    "duration_s": 19.477,
    "response_time_s": 19.477,
    "energy_wh_raw": 0.287502,
    "energy_wh_net": 0.287502,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q9",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A small integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Short factual query; minimal compute and output.",
    "tags": [
      "geography",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.595242,
    "energy_kwh_per_token": 5.95e-07,
    "g_co2": 0.100626
  },
  {
    "prompt": "When you mix red and blue paint, what color do you get?",
    "completion": "The user asks a simple question: \"When you mix red and blue paint, what color do you get?\" They likely want an answer. According to basic color mixing (subtractive), mixing red and blue gives purple or magenta depending on proportions; generally results in violet/magenta/purple. So answer concisely.\n</think>\nWhen you combine **red** and **blue** paint, you\u2019ll typically end up with a shade of **purple / magenta** (often called violet). The exact hue can vary depending on the specific reds and blues used\u2014more \u201cwarm\u201d reds tend to shift the mix toward orange\u2011pink, while cooler blues push it toward a bluish purple\u2014but in general the result is some form of purple.",
    "tokens_prompt": 30,
    "tokens_completion": 157,
    "duration_s": 6.868,
    "response_time_s": 6.868,
    "energy_wh_raw": 0.102433,
    "energy_wh_net": 0.102433,
    "provider": "ollama",
    "model": "nemotron-3-nano",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Linux with GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "device_type": "amd",
    "os_name": "Linux",
    "os_version": "6.14.0-37-generic",
    "cpu_model": "AMD RYZEN AI MAX+ 395 w/ Radeon 8060S",
    "gpu_model": "GPU[0]\t\t: Card Series: \t\tAMD Radeon Graphics",
    "ram_gb": 125.1,
    "chip_architecture": "x86_64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q10",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The resulting secondary color (purple or violet).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact with one\u2011word answer to establish baseline energy usage.",
    "tags": [
      "color-mixing",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.54777,
    "energy_kwh_per_token": 5.48e-07,
    "g_co2": 0.035852
  }
]