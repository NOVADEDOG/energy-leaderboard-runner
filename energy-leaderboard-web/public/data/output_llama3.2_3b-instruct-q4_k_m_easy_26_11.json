[
  {
    "prompt": "What is the capital city of Japan?",
    "completion": "The capital city of Japan is Tokyo.",
    "tokens_prompt": 33,
    "tokens_completion": 9,
    "duration_s": 2.957,
    "response_time_s": 2.957,
    "energy_wh_raw": 0.008,
    "energy_wh_net": 0.008,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q1",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A one\u2011word answer naming Japan\u2019s capital (Tokyo).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Very short prompt and answer provide a baseline for minimal energy consumption.",
    "tags": [
      "energy-baseline",
      "short-context",
      "factual"
    ],
    "wh_per_1k_tokens": 0.190476,
    "energy_kwh_per_token": 1.9e-07,
    "g_co2": 0.0028
  },
  {
    "prompt": "Calculate 7 + 5.",
    "completion": "7 + 5 = 12",
    "tokens_prompt": 32,
    "tokens_completion": 8,
    "duration_s": 0.37,
    "response_time_s": 0.37,
    "energy_wh_raw": 0.000778,
    "energy_wh_net": 0.000778,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q2",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single number representing the sum (12).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Simple arithmetic with minimal computation and output length.",
    "tags": [
      "numeric",
      "low-compute",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.01945,
    "energy_kwh_per_token": 1.9e-08,
    "g_co2": 0.000272
  },
  {
    "prompt": "Name the largest planet in our solar system.",
    "completion": "The largest planet in our solar system is Jupiter.",
    "tokens_prompt": 34,
    "tokens_completion": 11,
    "duration_s": 0.442,
    "response_time_s": 0.442,
    "energy_wh_raw": 0.001437,
    "energy_wh_net": 0.001437,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q3",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The name of the largest planet (Jupiter).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short factual recall requiring little reasoning.",
    "tags": [
      "astronomy",
      "factual",
      "energy-baseline"
    ],
    "wh_per_1k_tokens": 0.031933,
    "energy_kwh_per_token": 3.2e-08,
    "g_co2": 0.000503
  },
  {
    "prompt": "What is the chemical symbol for water?",
    "completion": "The chemical symbol for water is H2O. This indicates that one molecule of water consists of two hydrogen atoms (H) and one oxygen atom (O).",
    "tokens_prompt": 33,
    "tokens_completion": 33,
    "duration_s": 1.025,
    "response_time_s": 1.025,
    "energy_wh_raw": 0.006196,
    "energy_wh_net": 0.006196,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q4",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The two\u2011letter chemical formula (H2O).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short chemical knowledge question; minimal output.",
    "tags": [
      "chemistry",
      "short-output",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.093879,
    "energy_kwh_per_token": 9.4e-08,
    "g_co2": 0.002169
  },
  {
    "prompt": "Who wrote the play \u2018Romeo and Juliet\u2019?",
    "completion": "The play \"Romeo and Juliet\" was written by William Shakespeare. It is one of his most famous tragedies, first published in 1597 and believed to have been written between 1591 and 1595.",
    "tokens_prompt": 37,
    "tokens_completion": 46,
    "duration_s": 1.373,
    "response_time_s": 1.372,
    "energy_wh_raw": 0.009072,
    "energy_wh_net": 0.009072,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q5",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The author\u2019s name (William Shakespeare).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact retrieval; low energy.",
    "tags": [
      "literature",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.109301,
    "energy_kwh_per_token": 1.09e-07,
    "g_co2": 0.003175
  },
  {
    "prompt": "In which year did humans first land on the Moon?",
    "completion": "Humans first landed on the Moon in 1969. The Apollo 11 mission, crewed by astronauts Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins, successfully landed on July 20, 1969.",
    "tokens_prompt": 36,
    "tokens_completion": 47,
    "duration_s": 1.376,
    "response_time_s": 1.376,
    "energy_wh_raw": 0.008843,
    "energy_wh_net": 0.008843,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q6",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A four\u2011digit year (1969).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Short question with numeric answer; minimal compute.",
    "tags": [
      "history",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.106542,
    "energy_kwh_per_token": 1.07e-07,
    "g_co2": 0.003095
  },
  {
    "prompt": "Which element has the atomic number 6?",
    "completion": "The element with atomic number 6 is Carbon (C).",
    "tokens_prompt": 34,
    "tokens_completion": 13,
    "duration_s": 0.503,
    "response_time_s": 0.503,
    "energy_wh_raw": 0.001522,
    "energy_wh_net": 0.001522,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q7",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The element\u2019s name (Carbon).",
    "max_output_tokens_hint": 5,
    "energy_relevance": "Simple factual recall; low energy cost.",
    "tags": [
      "chemistry",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.032383,
    "energy_kwh_per_token": 3.2e-08,
    "g_co2": 0.000533
  },
  {
    "prompt": "What is the square root of 49?",
    "completion": "The square root of 49 is 7.",
    "tokens_prompt": 34,
    "tokens_completion": 11,
    "duration_s": 0.434,
    "response_time_s": 0.434,
    "energy_wh_raw": 0.001462,
    "energy_wh_net": 0.001462,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q8",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A single integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Basic arithmetic with one\u2011word output.",
    "tags": [
      "math",
      "numeric",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.032489,
    "energy_kwh_per_token": 3.2e-08,
    "g_co2": 0.000512
  },
  {
    "prompt": "How many continents are there on Earth?",
    "completion": "There is ongoing debate and no consensus among geographers, scientists, and scholars about the number of continents on Earth. Traditionally, the seven-continent model has been widely accepted:\n\n1. Africa\n2. Antarctica\n3. Asia\n4. Australia\n5. Europe\n6. North America\n7. South America\n\nHowever, some researchers argue that the continent of Europe should be combined with Asia to form a single supercontinent called Eurasia, which would leave us with six continents.\n\nOthers propose that the seven-continent model is outdated and that the continents are not distinct landmasses, but rather collections of geological features and plate boundaries. In this view, there are only four or five major continental regions:\n\n1. Afro-Eurasia (Africa + Asia)\n2. Americas (North America + South America)\n3. Australia\n4. Antarctica\n\nUltimately, the number of continents is a matter of definition, classification, and interpretation. There is no universally accepted answer.",
    "tokens_prompt": 33,
    "tokens_completion": 198,
    "duration_s": 5.358,
    "response_time_s": 5.358,
    "energy_wh_raw": 0.041277,
    "energy_wh_net": 0.041277,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q9",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "A small integer (7).",
    "max_output_tokens_hint": 4,
    "energy_relevance": "Short factual query; minimal compute and output.",
    "tags": [
      "geography",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.178688,
    "energy_kwh_per_token": 1.79e-07,
    "g_co2": 0.014447
  },
  {
    "prompt": "When you mix red and blue paint, what color do you get?",
    "completion": "When you mix red and blue paint, you get the color purple. The exact shade of purple may vary depending on the ratio of red to blue that you use, but in general, mixing red and blue creates a range of purple hues.",
    "tokens_prompt": 39,
    "tokens_completion": 49,
    "duration_s": 1.454,
    "response_time_s": 1.454,
    "energy_wh_raw": 0.009311,
    "energy_wh_net": 0.009311,
    "provider": "ollama",
    "model": "llama3.2:3b-instruct-q4_k_m",
    "region": "unknown",
    "notice": null,
    "sampling_ms": 100,
    "device_name": "Apple M1 Pro Mac (MacBookPro18,3)",
    "device_type": "apple",
    "os_name": "macOS",
    "os_version": "26.1",
    "cpu_model": "Apple M1 Pro",
    "gpu_model": "Apple M1 Pro",
    "ram_gb": 16.0,
    "chip_architecture": "arm64",
    "testset_id": "ts1",
    "testset_name": "Short factual questions (low energy baseline)",
    "testset_goal": "Establish a low\u2011energy baseline by asking simple factual or arithmetic questions that require short, single\u2011sentence answers.",
    "testset_notes": "Use this set to measure the minimum energy cost of a local LLM under controlled conditions. Each prompt is concise, and the expected output is a short factual answer with minimal reasoning. Run multiple models on the same hardware and compare average energy per response.",
    "question_id": "ts1_q10",
    "question_difficulty": "easy",
    "question_task_type": "qa",
    "expected_answer_description": "The resulting secondary color (purple or violet).",
    "max_output_tokens_hint": 6,
    "energy_relevance": "Simple fact with one\u2011word answer to establish baseline energy usage.",
    "tags": [
      "color-mixing",
      "factual",
      "baseline"
    ],
    "wh_per_1k_tokens": 0.105807,
    "energy_kwh_per_token": 1.06e-07,
    "g_co2": 0.003259
  }
]